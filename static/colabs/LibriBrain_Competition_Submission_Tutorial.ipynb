{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "âš ï¸ **Note**: **Submission is currently non-functional, as the obfuscated holdout data has not been released yet. This will be resolved in time for the start of the competition.**"
      ],
      "metadata": {
        "id": "xdEBpx2vchzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ LibriBrain Competition: Submission\n",
        "You've trained a model for one of our tracks and are now ready to submit your results? Congratulations! - let's walk through the process.\n",
        "\n",
        "Broadly, you will need to do the following:\n",
        "1. Register your team on Eval.ai (see the tutorial [here](https://evalai.readthedocs.io/en/latest/participate.html))\n",
        "2. Run model predictions on our holdout data\n",
        "3. Submit the .CSV file containing your results.\n",
        "\n",
        "This tutorial will walk you through step (2), generating the .CSV file for you to submit.\n",
        "\n",
        "In case of any questions or problems, please get in touch through [our Discord server](https://discord.gg/Fqr8gJnvSh).\n",
        "\n",
        "âš ï¸ **Note**: We have only comprehensively validated the notebook to work on Colab and Unix. Your experience in other environments (e.g., Windows) may vary."
      ],
      "metadata": {
        "id": "lhfbDfTYqMf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up dependencies\n",
        "Run the code below *as is*. It will download all required dependencies, including our own [PNPL](https://pypi.org/project/pnpl/) package. On Windows, you might have to restart your Kernel after the installation has finished."
      ],
      "metadata": {
        "id": "LZTwXzPRtP6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install additional dependencies\n",
        "%pip install -q mne_bids lightning torchmetrics scikit-learn plotly ipywidgets pnpl\n",
        "\n",
        "# Set up base path for dataset and related files (base_path is assumed to be set in the cells below!)\n",
        "base_path = \"./libribrain\"\n",
        "try:\n",
        "    import google.colab  # This module is only available in Colab.\n",
        "    in_colab = True\n",
        "    base_path = \"/content\"  # This is the folder displayed in the Colab sidebar\n",
        "except ImportError:\n",
        "    in_colab = False"
      ],
      "metadata": {
        "id": "7WXzSJx9srGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading our model\n",
        "The code below sets up the model architecture of our two baselines models and loads pretrained weights (so we can skip training).\n",
        "In your own submission, this is where you would swap in your own setup.\n",
        "While we are demonstrating submission for both tracks (Speech Detection and Phoneme Classification), you will only need to set up a single model for one of the tracks here."
      ],
      "metadata": {
        "id": "ym2S4eqktZsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import lightning as L\n",
        "import torchmetrics\n",
        "import requests\n",
        "import os\n",
        "import numpy as np\n",
        "from lightning.pytorch.callbacks import Callback\n",
        "from sklearn.metrics import roc_curve, auc, balanced_accuracy_score, jaccard_score\n",
        "\n",
        "# Custom BCE loss with label smoothing\n",
        "class BCEWithLogitsLossWithSmoothing(nn.Module):\n",
        "    def __init__(self, smoothing=0.1, pos_weight=1.0):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]))\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        target = target.float()  # ensure target is float\n",
        "        target_smoothed = target * (1 - self.smoothing) + self.smoothing * 0.5\n",
        "        return self.bce_loss(logits, target_smoothed)\n",
        "\n",
        "#\n",
        "# Speech Detection Model\n",
        "#\n",
        "class SpeechDetectionModel(L.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Fixed architecture parameters\n",
        "        input_dim = 23         # e.g. len(SENSORS_SPEECH_MASK)\n",
        "        model_dim = 100\n",
        "        dropout_rate = 0.5\n",
        "        lstm_layers = 2\n",
        "        bi_directional = False\n",
        "        batch_norm = False\n",
        "\n",
        "        # Define convolutional block\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels=input_dim,\n",
        "            out_channels=model_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.batch_norm = nn.BatchNorm1d(model_dim) if batch_norm else nn.Identity()\n",
        "        self.conv_dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Define LSTM block (batch_first=True so that output shape is (batch, seq_len, features))\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=model_dim,\n",
        "            hidden_size=model_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            dropout=dropout_rate,\n",
        "            batch_first=True,\n",
        "            bidirectional=bi_directional\n",
        "        )\n",
        "        self.lstm_dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Final classifier layer\n",
        "        self.speech_classifier = nn.Linear(model_dim, 1)\n",
        "\n",
        "        # Use our loss with smoothing\n",
        "        self.loss_fn = BCEWithLogitsLossWithSmoothing(smoothing=0.1, pos_weight=1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, channels, seq_len)\n",
        "        x = self.conv(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.conv_dropout(x)\n",
        "        # Permute to (batch, seq_len, features) for LSTM\n",
        "        x = x.permute(0, 2, 1)\n",
        "        output, (h_n, c_n) = self.lstm(x)\n",
        "        # Use the last hidden state from the final LSTM layer (shape: (batch, model_dim))\n",
        "        hidden = h_n[-1]\n",
        "        hidden = self.lstm_dropout(hidden)\n",
        "        logits = self.speech_classifier(hidden)\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch  # assume y is (batch,) with binary labels\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y.unsqueeze(1).float())\n",
        "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y.unsqueeze(1).float())\n",
        "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y.unsqueeze(1).float())\n",
        "        self.log(\"test_loss\", loss, prog_bar=True)\n",
        "        return {\"loss\": loss, \"logits\": logits, \"y\": y}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3, weight_decay=0.01)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "#\n",
        "# Phoneme Classification Model\n",
        "#\n",
        "class PhonemeClassificationModel(L.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv1d(306, 128, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16000, 39)\n",
        "        )\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.f1_macro = torchmetrics.F1Score(num_classes=39, average='macro', task=\"multiclass\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        f1_macro = self.f1_macro(y_hat, y)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_f1_macro', f1_macro)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        f1_macro = self.f1_macro(y_hat, y)\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_f1_macro', f1_macro, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.model.parameters(), lr=0.0005)\n",
        "\n",
        "\n",
        "# Set a fixed seed for reproducibility and initialize trainer\n",
        "L.seed_everything(42)\n",
        "trainer = L.Trainer(devices=\"auto\")\n",
        "\n",
        "CHECKPOINT_PATH_SPEECH = f\"{base_path}/models/speech_model_minimal.ckpt\"\n",
        "CHECKPOINT_PATH_PHONEME = f\"{base_path}/models/phoneme_model.ckpt\"\n",
        "\n",
        "# Download and load the speech model checkpoint if it does not already exist\n",
        "if not os.path.exists(CHECKPOINT_PATH_SPEECH):\n",
        "    speech_model_url = \"https://neural-processing-lab.github.io/2025-libribrain-competition/speech_model_minimal.ckpt\"\n",
        "    response = requests.get(speech_model_url)\n",
        "    os.makedirs(os.path.dirname(CHECKPOINT_PATH_SPEECH), exist_ok=True)\n",
        "    with open(CHECKPOINT_PATH_SPEECH, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Download of speech model checkpoint complete.\")\n",
        "else:\n",
        "    print(\"Speech model checkpoint already exists. Skipping download.\")\n",
        "\n",
        "# Load the unified SpeechDetectionModel from checkpoint\n",
        "speech_model = SpeechDetectionModel.load_from_checkpoint(\n",
        "    checkpoint_path=CHECKPOINT_PATH_SPEECH\n",
        ")\n",
        "\n",
        "# Download and load the phoneme model checkpoint if it does not already exist\n",
        "if not os.path.exists(CHECKPOINT_PATH_PHONEME):\n",
        "    phoneme_model_url = \"https://neural-processing-lab.github.io/2025-libribrain-competition/phoneme_model.ckpt\"\n",
        "    response = requests.get(phoneme_model_url)\n",
        "    os.makedirs(os.path.dirname(CHECKPOINT_PATH_PHONEME), exist_ok=True)\n",
        "    with open(CHECKPOINT_PATH_PHONEME, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Download of phoneme model checkpoint complete.\")\n",
        "else:\n",
        "    print(\"Phoneme model checkpoint already exists. Skipping download.\")\n",
        "\n",
        "# Load the PhonemeClassificationModel from checkpoint\n",
        "phoneme_model = PhonemeClassificationModel.load_from_checkpoint(\n",
        "    checkpoint_path=CHECKPOINT_PATH_PHONEME\n",
        ")\n"
      ],
      "metadata": {
        "id": "l_4X486cuUnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating submission file\n",
        "Now that we have our models loaded, let's generate the submission CSV."
      ],
      "metadata": {
        "id": "aK_kau-UtfAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Speech detection submission"
      ],
      "metadata": {
        "id": "9E7dho4jjDax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightning as L\n",
        "import requests\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from pnpl.datasets import LibriBrainCompetitionHoldout\n",
        "\n",
        "\n",
        "holdout_dataset = LibriBrainCompetitionHoldout(base_path, task=\"speech\")\n",
        "holdout_loader = DataLoader(holdout_dataset, batch_size=1, num_workers=0,shuffle=False)\n",
        "\n",
        "# Generate predictions for the holdout dataset\n",
        "predictions = trainer.predict(speech_model, dataloaders=holdout_loader)\n",
        "# Submit predictions to CSV\n",
        "holdout_dataset.generate_submission_in_csv(predictions=predictions, output_path=\"speech_predictions.csv\")"
      ],
      "metadata": {
        "id": "ngk-x8MsteJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phoneme classification submission"
      ],
      "metadata": {
        "id": "r2n5zcSSjHtM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyHI1WA7qH6h"
      },
      "outputs": [],
      "source": [
        "import lightning as L\n",
        "import requests\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from pnpl.datasets import LibriBrainCompetitionHoldout\n",
        "\n",
        "\n",
        "holdout_dataset = LibriBrainCompetitionHoldout(base_path, task=\"phoneme\")\n",
        "holdout_loader = DataLoader(holdout_dataset, batch_size=1, num_workers=0,shuffle=False)\n",
        "\n",
        "# Generate predictions for the holdout dataset\n",
        "predictions = trainer.predict(phoneme_model, dataloaders=holdout_loader)\n",
        "# Submit predictions to CSV\n",
        "holdout_dataset.generate_submission_in_csv(predictions=predictions, output_path=\"phoneme_predictions.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## That's it! ðŸ¥³\n",
        "You've successfully generated a submission .CSV file. Two steps remain:\n",
        "1. Download the file by selecting `Files -> speech_predictions.csv -> Download` or `Files -> phoneme_predications.csv -> Download` on the toolbar on the left\n",
        "2. Upload the file on EvalAI to secure your spot on the leaderboard.\n",
        "\n",
        "If you have any open questions, get in touch on [our Discord server](https://discord.gg/Fqr8gJnvSh) or find more information on [our website](https://neural-processing-lab.github.io/2025-libribrain-competition). Good luck!"
      ],
      "metadata": {
        "id": "1uh_qxGged7-"
      }
    }
  ]
}