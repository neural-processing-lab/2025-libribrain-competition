{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdEBpx2vchzI"
      },
      "source": [
        "‚ö†Ô∏è **Note**: **Submission is currently limited to only the speech detection tasks. We'll be releasing the obfuscated holdout data and an updated submission tutorial for the Phoneme Classification tasks in time for the second half of the competition.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhfbDfTYqMf2"
      },
      "source": [
        "# üçç LibriBrain Competition: Submission\n",
        "You've trained a model for one of our tracks and are now ready to submit your results? Congratulations! - let's walk through the process.\n",
        "\n",
        "Broadly, you will need to do the following:\n",
        "1. Run model predictions on our holdout data\n",
        "2. Submit the .CSV file containing your results (find the detailed instructions [here](https://neural-processing-lab.github.io/2025-libribrain-competition/participate/#4-submit-on-evalai)).\n",
        "\n",
        "This tutorial will walk you through step (1), generating the .CSV file for you to submit.\n",
        "\n",
        "In case of any questions or problems, please get in touch through [our Discord server](https://neural-processing-lab.github.io/2025-libribrain-competition/links/discord).\n",
        "\n",
        "‚ö†Ô∏è **Note**: We have only comprehensively validated the notebook to work on Colab and Unix. Your experience in other environments (e.g., Windows) may vary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZTwXzPRtP6J"
      },
      "source": [
        "## Setting up dependencies\n",
        "Run the code below *as is*. It will download all required dependencies, including our own [PNPL](https://pypi.org/project/pnpl/) package. On Windows, you might have to restart your Kernel after the installation has finished."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WXzSJx9srGY"
      },
      "outputs": [],
      "source": [
        "# Install additional dependencies\n",
        "%pip install -q lightning torchmetrics scikit-learn plotly ipywidgets tqdm pnpl\n",
        "\n",
        "# Set up base path for dataset and related files (base_path is assumed to be set in the cells below!)\n",
        "base_path = \"./libribrain\"\n",
        "try:\n",
        "    import google.colab  # This module is only available in Colab.\n",
        "    in_colab = True\n",
        "    base_path = \"/content\"  # This is the folder displayed in the Colab sidebar\n",
        "except ImportError:\n",
        "    in_colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXJlK9ZhpUjM"
      },
      "source": [
        "## Generating submission CSV\n",
        "For the speech detection task, you will be asked to evaluate **for each sample** of the \"competition holdout\" split of the data if it is speech or not - this means we expect a total of 560,638 predictions (that is how many samples there are in that split). These predictions should then be packaged into a .csv file you can upload on EvalAI. As we don't have labels to train against, the way you download the holdout data differs slightly from the regular `LibriBrainSpeech` dataset.\n",
        "\n",
        "Here is how to generate the submission:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TePs7dQRCbVd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from pnpl.datasets import LibriBrainCompetitionHoldout\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# First, instantiate the Competition Holdout dataset\n",
        "speech_holdout_dataset = LibriBrainCompetitionHoldout(\n",
        "    data_path=base_path,  # Same as in the other LibriBrain dataset - this is where we'll store the data\n",
        "    tmax=0.8,             # Also identical to the other datasets - how many samples to return/group together\n",
        "    task=\"speech\"         # \"speech\" or \"phoneme\" (\"phoneme\" is not supported until Phoneme track launch)\n",
        ")\n",
        "\n",
        "# Next, create a DataLoader for the dataset\n",
        "dataloader = DataLoader(\n",
        "    speech_holdout_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=4   # Increase workers to speed up sample loading\n",
        ")\n",
        "\n",
        "# The final array of predictions must contain len(speech_holdout_dataset) values between 0..1\n",
        "segments_to_predict = len(speech_holdout_dataset)\n",
        "print(segments_to_predict)\n",
        "\n",
        "# Finally, we loop over every sample to generate a prediction.\n",
        "# For now, we will fill the submission with random values\n",
        "all_random = torch.rand((segments_to_predict, 1))\n",
        "random_predictions = [None] * segments_to_predict\n",
        "\n",
        "for i, sample in enumerate(tqdm(dataloader)):\n",
        "    # For your submission, this is where you would generate your model prediction:\n",
        "    # segment = sample[0]                  # The actual segment data is at sample[0]\n",
        "    # prediction = model.predict(segment)  # Assuming model has a predict method\n",
        "    #\n",
        "    # Here, we simply pull the precomputed random tensor instead\n",
        "    random_predictions[i] = all_random[i]\n",
        "\n",
        "speech_holdout_dataset.generate_submission_in_csv(\n",
        "    random_predictions,\n",
        "    \"holdout_speech_predictions.csv\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeNyLjZWEyFS"
      },
      "source": [
        "If you don't wish to wait the ~20min it takes to generate the file above, you can generate a mock (valid, but filled with random values) submission file without iterating over all samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2fpYCXLExb6"
      },
      "outputs": [],
      "source": [
        "from pnpl.datasets import LibriBrainCompetitionHoldout\n",
        "import torch\n",
        "\n",
        "\n",
        "speech_holdout_dataset = LibriBrainCompetitionHoldout(\n",
        "    data_path=base_path,\n",
        "    tmax=0.8,\n",
        "    task=\"speech\"\n",
        ")\n",
        "\n",
        "segments_to_predict = len(speech_holdout_dataset)\n",
        "all_random = torch.rand((segments_to_predict, 1))  # build all (1,) tensors at once\n",
        "random_predictions = list(all_random)              # convert to list of shape-(1,) tensors\n",
        "speech_holdout_dataset.generate_submission_in_csv(\n",
        "    random_predictions,\n",
        "    \"holdout_speech_predictions.csv\"\n",
        ")\n",
        "print(\"Submission file created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiaqfLF3med8"
      },
      "source": [
        "### Generating the correct number of predictions\n",
        "The code above is all you need to generate your submission!\n",
        "\n",
        "We understand that while training your model, you may have played around with averaging samples, combining multiple samples into a singular output - in fact, the baseline model used in the [Speech Detection Notebook](https://neural-processing-lab.github.io/2025-libribrain-competition/colabs/LibriBrain_Competition_Speech_Detection.ipynb) did just that.\n",
        "\n",
        "But, for the submission to be valid, it will need to contain 560,638 predictions - one per sample. There are multiple ways to resolve this (predicting a baseline value if no prediction can be performed, interpolating between results,...)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ready to submit?\n",
        "After generating the predictions file, the next step is to submit it for evaluation. Don't worry, you are allowed to submit multiple times. Please, take a look at the [Submit on EvalAI](https://neural-processing-lab.github.io/2025-libribrain-competition/participate/#4-submit-on-evalai) section on the website to learn more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDGZ8LLzAMvb"
      },
      "source": [
        "## That's it! ü•≥\n",
        "Thanks for taking the time to look at and/or participate in our competition. If this caught your interest, you might also want to take a look at the more advanced version of the task, focussed on Phoneme Classification - you can find the corresponding Colab [here](https://neural-processing-lab.github.io/2025-libribrain-competition/links/phoneme-colab). If you have any open questions, please get in touch through [our Discord server](https://neural-processing-lab.github.io/2025-libribrain-competition/links/discord)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
