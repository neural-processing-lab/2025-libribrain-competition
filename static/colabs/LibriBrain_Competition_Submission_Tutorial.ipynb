{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "‚ö†Ô∏è **Note**: **Submission is currently limited to only the speech detection tasks. We'll be releasing the obfuscated holdout data and an updated submission tutorial for the Phoneme Classification tasks in time for the second half of the competition.**"
      ],
      "metadata": {
        "id": "xdEBpx2vchzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçç LibriBrain Competition: Submission\n",
        "You've trained a model for one of our tracks and are now ready to submit your results? Congratulations! - let's walk through the process.\n",
        "\n",
        "Broadly, you will need to do the following:\n",
        "1. Register your team on Eval.ai (see the tutorial [here](https://evalai.readthedocs.io/en/latest/participate.html))\n",
        "2. Run model predictions on our holdout data\n",
        "3. Submit the .CSV file containing your results.\n",
        "\n",
        "This tutorial will walk you through step (2), generating the .CSV file for you to submit.\n",
        "\n",
        "In case of any questions or problems, please get in touch through [our Discord server](https://neural-processing-lab.github.io/2025-libribrain-competition/links/discord).\n",
        "\n",
        "‚ö†Ô∏è **Note**: We have only comprehensively validated the notebook to work on Colab and Unix. Your experience in other environments (e.g., Windows) may vary."
      ],
      "metadata": {
        "id": "lhfbDfTYqMf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up dependencies\n",
        "Run the code below *as is*. It will download all required dependencies, including our own [PNPL](https://pypi.org/project/pnpl/) package. On Windows, you might have to restart your Kernel after the installation has finished."
      ],
      "metadata": {
        "id": "LZTwXzPRtP6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install additional dependencies\n",
        "%pip install -q mne_bids lightning torchmetrics scikit-learn plotly ipywidgets tqdm pnpl\n",
        "\n",
        "# Set up base path for dataset and related files (base_path is assumed to be set in the cells below!)\n",
        "base_path = \"./libribrain\"\n",
        "try:\n",
        "    import google.colab  # This module is only available in Colab.\n",
        "    in_colab = True\n",
        "    base_path = \"/content\"  # This is the folder displayed in the Colab sidebar\n",
        "except ImportError:\n",
        "    in_colab = False"
      ],
      "metadata": {
        "id": "7WXzSJx9srGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating submission CSV\n",
        "For the speech detection task, you will be asked to evaluate **for each sample** of the \"competition holdout\" split of the data if it is speech or not - this means we expect a total of 560,638 predictions (that is how many samples there are in that split). These predictions should then be packaged into a .csv file you can upload on EvalAI. As we don't have labels to train against, the way you download the holdout data differs slightly from the regular `LibriBrainSpeech` dataset.\n",
        "\n",
        "Here is how to generate the submission:"
      ],
      "metadata": {
        "id": "nXJlK9ZhpUjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from pnpl.datasets import LibriBrainCompetitionHoldout\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# First, instantiate the Competition Holdout dataset\n",
        "speech_holdout_dataset = LibriBrainCompetitionHoldout(\n",
        "    data_path = base_path,  # Same as in the other LibriBrain dataset - this is where we'll store the data\n",
        "    tmax=0.8,  # Also identical to the other datasets - how many samples to return group together (e.g., if combining multiple samples).\n",
        "    task=\"speech\"  # \"speech\" or \"phoneme\" (\"phoneme\" is not supported until the launch of the Phoneme Classification track!)\n",
        ")\n",
        "\n",
        "# Next, create a DataLoader for the dataset\n",
        "dataloader = DataLoader(speech_holdout_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "# The final array of predictions must contain len(speech_holdout_dataset) values between 0..1\n",
        "segments_to_predict = len(speech_holdout_dataset)\n",
        "print(segments_to_predict)\n",
        "\n",
        "# For now, we will fill it with random values\n",
        "# This may take a moment, so we wrap it with a progress bar\n",
        "random_predictions = []\n",
        "for i, sample in enumerate(tqdm(dataloader)):\n",
        "    # For your submission, this is where you would generate your model prediction\n",
        "    # segment = sample[0]  # The actual segment data is at sample[0]\n",
        "    # prediction = model.predict(segment)  # Assuming model is defined and has a predict method\n",
        "    random_predictions.append(torch.rand(1))  # Random prediction for each sample\n",
        "speech_holdout_dataset.generate_submission_in_csv(random_predictions, \"holdout_speech_predictions.csv\")"
      ],
      "metadata": {
        "id": "vFtvfEbGpsCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating the correct number of predictions\n",
        "The code above is all you need to generate your submission!\n",
        "\n",
        "We understand that while training your model, you may have played around with averaging samples, combining multiple samples into a singular output - in fact, the baseline model used in the [Speech Detection Notebook](https://neural-processing-lab.github.io/2025-libribrain-competition/colabs/LibriBrain_Competition_Speech_Detection.ipynb) did just that.\n",
        "\n",
        "But, for the submission to be valid, it will need to contain 560,638 predictions - one per sample. There are multiple ways to resolve this (predicting a baseline value if no prediction can be performed, interpolating between results,...)."
      ],
      "metadata": {
        "id": "EiaqfLF3med8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDGZ8LLzAMvb"
      },
      "source": [
        "## That's it! ü•≥\n",
        "Thanks for taking the time to look at and/or participate in our competition. If this caught your interest, you might also want to take a look at the more advanced version of the task, focussed on Phoneme Classification - you can find the corresponding Colab [here](https://neural-processing-lab.github.io/2025-libribrain-competition/links/phoneme-colab). If you have any open questions, please get in touch through [our Discord server](https://neural-processing-lab.github.io/2025-libribrain-competition/links/discord). Ready to submit your model? You might want to take a look at the [submission tutorial](https://neural-processing-lab.github.io/2025-libribrain-competition/links/submission-colab) or the [LibriBrain competition website](https://neural-processing-lab.github.io/2025-libribrain-competition)."
      ]
    }
  ]
}