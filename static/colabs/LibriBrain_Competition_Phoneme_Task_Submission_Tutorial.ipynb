{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhfbDfTYqMf2"
      },
      "source": [
        "# üçç LibriBrain Competition: Phoneme Classification Submission\n",
        "You've trained a model for the Phoneme Classification track and are now ready to submit your results? Congratulations! - let's walk through the process.\n",
        "\n",
        "Broadly, you will need to do the following:\n",
        "1. Run model predictions on our holdout data\n",
        "2. Submit the .CSV file containing your results (find the detailed instructions [here](https://neural-processing-lab.github.io/2025-libribrain-competition/participate/#4-submit-on-evalai)).\n",
        "\n",
        "This tutorial will walk you through step (1), generating the .CSV file for you to submit.\n",
        "\n",
        "In case of any questions or problems, please get in touch through [our Discord server](https://neural-processing-lab.github.io/2025-libribrain-competition/links/discord).\n",
        "\n",
        "‚ö†Ô∏è **Note**: We have only comprehensively validated the notebook to work on Colab and Unix. Your experience in other environments (e.g., Windows) may vary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZTwXzPRtP6J"
      },
      "source": [
        "## Setting up dependencies\n",
        "Run the code below *as is*. It will download all required dependencies, including our own [PNPL](https://pypi.org/project/pnpl/) package. On Windows, you might have to restart your Kernel after the installation has finished."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WXzSJx9srGY"
      },
      "outputs": [],
      "source": [
        "# Install additional dependencies\n",
        "%pip install -q lightning torchmetrics scikit-learn plotly ipywidgets tqdm pnpl\n",
        "\n",
        "# Set up base path for dataset and related files (base_path is assumed to be set in the cells below!)\n",
        "base_path = \"./libribrain\"\n",
        "try:\n",
        "    import google.colab  # This module is only available in Colab.\n",
        "    in_colab = True\n",
        "    base_path = \"/content\"  # This is the folder displayed in the Colab sidebar\n",
        "except ImportError:\n",
        "    in_colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Things to note about the holdout data\n",
        "The input data consists of MEG segments with shape `(306, 125)` - that's 306 MEG channels over 125 time points (0.5 seconds at 250Hz sampling rate). Your model should process these segments and output a probability distribution over 39 phoneme classes.\n",
        "\n",
        "**‚ö†Ô∏è The 125 timepoints represent the time around the phoneme using tmin=0 and tmax=0.5, meaning you get the signal immediately upon start of the phoneme and the 0.5 seconds following it.**\n",
        "\n",
        "The majority of samples are created by signal averaging 100 samples. For some phonemes, fewer than 100 samples were available, so they were averaged on less datapoints."
      ],
      "metadata": {
        "id": "yw9Lkq9Qmgzj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXJlK9ZhpUjM"
      },
      "source": [
        "## Generating submission CSV\n",
        "For the phoneme classification task, you will be asked to classify **each segment** of the \"competition holdout\" split into one of 39 phoneme categories. Each prediction should be a probability distribution over the 39 phonemes, so your model should output 39 normalized probabilities that sum to 1.0. These predictions should then be packaged into a .csv file you can upload on EvalAI.\n",
        "\n",
        "\n",
        "\n",
        "Here is how to generate the submission:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TePs7dQRCbVd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from pnpl.datasets import LibriBrainCompetitionHoldout\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# First, instantiate the Competition Holdout dataset for phoneme classification\n",
        "phoneme_holdout_dataset = LibriBrainCompetitionHoldout(\n",
        "    data_path=base_path,  # Same as in the other LibriBrain dataset - this is where we'll store the data\n",
        "    task=\"phoneme\"        # Use \"phoneme\" for phoneme classification task\n",
        ")\n",
        "\n",
        "print(f\"Dataset loaded: {len(phoneme_holdout_dataset)} segments\")\n",
        "print(f\"Each segment shape: {phoneme_holdout_dataset[0].shape}\")  # Should be (306, 125)\n",
        "\n",
        "# Next, create a DataLoader for the dataset\n",
        "dataloader = DataLoader(\n",
        "    phoneme_holdout_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=4   # Increase workers to speed up sample loading\n",
        ")\n",
        "\n",
        "# The final array of predictions must contain len(phoneme_holdout_dataset) 39-dimensional probability vectors\n",
        "segments_to_predict = len(phoneme_holdout_dataset)\n",
        "print(f\"Total segments to predict: {segments_to_predict}\")\n",
        "\n",
        "# For now, we will fill the submission with random probability distributions\n",
        "# Each prediction should be a 39-dimensional probability vector (one for each phoneme)\n",
        "random_predictions = []\n",
        "\n",
        "for i, sample in enumerate(tqdm(dataloader)):\n",
        "    # For your submission, this is where you would generate your model prediction:\n",
        "    # segment = sample  # The actual segment data - shape (306, 125)\n",
        "    # logits = model(segment)  # Your model outputs logits of shape (39,)\n",
        "    # probabilities = torch.softmax(logits, dim=0)  # Convert to probabilities\n",
        "    #\n",
        "    # Here, we generate random probabilities instead\n",
        "    random_logits = torch.randn(39)  # Random logits for 39 phoneme classes\n",
        "    random_probs = torch.softmax(random_logits, dim=0)  # Convert to probabilities\n",
        "    random_predictions.append(random_probs)\n",
        "\n",
        "# Generate the submission CSV\n",
        "phoneme_holdout_dataset.generate_submission_in_csv(\n",
        "    random_predictions,\n",
        "    \"holdout_phoneme_predictions.csv\"\n",
        ")\n",
        "print(\"Submission file created: holdout_phoneme_predictions.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7wym0wqjZ8a"
      },
      "source": [
        "## Using a real PyTorch model\n",
        "\n",
        "Now let's use the simple model introduced in the tutorial notebook to generate a submission!\n",
        "\n",
        "In order to achieve that, we'll\n",
        "1. Define the model architecture (identical to tutorial notebook)\n",
        "2. Download and instantiate a pre-trained checkpoint\n",
        "3. Use the instantiated model to generate a valid submission CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w2tKYPbjZ8a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import lightning as L\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torchmetrics import F1Score\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "class PhonemeClassificationModel(L.LightningModule):\n",
        "    \"\"\"\n",
        "    Lightning model for phoneme classification from MEG data (identical to tutorial notebook!)\n",
        "\n",
        "    Architecture:\n",
        "    - Conv1d: 306 channels -> 128 channels (1x1 convolution)\n",
        "    - ReLU activation\n",
        "    - Flatten: (128, 125) -> (16000,)\n",
        "    - Linear: 16000 -> 39 phoneme classes\n",
        "\n",
        "    Input: (batch_size, 306, 125) - 306 MEG channels, 125 time points\n",
        "    Output: (batch_size, 39) - logits for 39 phoneme classes\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv1d(306, 128, 1),  # 1x1 convolution\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),            # (128, 125) -> (16000,)\n",
        "            nn.Linear(16000, 39)     # 39 phoneme classes\n",
        "        )\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.f1_macro = F1Score(num_classes=39, average='macro', task=\"multiclass\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        f1_macro = self.f1_macro(y_hat, y)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_f1_macro', f1_macro)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        f1_macro = self.f1_macro(y_hat, y)\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_f1_macro', f1_macro, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.model.parameters(), lr=0.0005)\n",
        "\n",
        "# Load the trained model from checkpoint\n",
        "checkpoint_path = f\"{base_path}/phoneme_model.ckpt\"\n",
        "\n",
        "\n",
        "# Download model if it doesn't exist\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    print(f\"Downloading model to {checkpoint_path}\")\n",
        "    url = \"https://neural-processing-lab.github.io/2025-libribrain-competition/phoneme_model.ckpt\"\n",
        "    urllib.request.urlretrieve(url, checkpoint_path)\n",
        "    print(\"Model downloaded successfully!\")\n",
        "\n",
        "\n",
        "print(f\"Loading trained model from: {checkpoint_path}\")\n",
        "try:\n",
        "    # Load the trained Lightning model\n",
        "    model = PhonemeClassificationModel.load_from_checkpoint(checkpoint_path)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"‚úÖ Model loaded successfully!\")\n",
        "    print(f\"üìä Model has {total_params:,} parameters\")\n",
        "\n",
        "    # Test with a single sample\n",
        "    test_input = torch.randn(1, 306, 125)\n",
        "    with torch.no_grad():\n",
        "        test_output = model(test_input)\n",
        "        test_probs = torch.softmax(test_output, dim=1)\n",
        "        print(f\"‚úì Test output shape: {test_output.shape}\")\n",
        "        print(f\"‚úì Test probabilities sum to: {test_probs.sum().item():.6f}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Checkpoint not found at: {checkpoint_path}\")\n",
        "    print(\"Creating a randomly initialized model for demonstration\")\n",
        "    model = PhonemeClassificationModel()\n",
        "    model.eval()\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Random model has {total_params:,} parameters\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading checkpoint: {e}\")\n",
        "    print(\"Creating a randomly initialized model for demonstration\")\n",
        "    model = PhonemeClassificationModel()\n",
        "    model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIPn0jjljZ8a"
      },
      "outputs": [],
      "source": [
        "def generate_model_predictions(model, dataset, batch_size=32, device='cpu'):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Create data loader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    print(f\"Generating predictions for {len(dataset)} segments...\")\n",
        "    print(f\"Using batch size: {batch_size}, device: {device}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=\"Processing batches\")):\n",
        "            # batch_data shape: (batch_size, 306, 125)\n",
        "            batch_data = batch_data.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(batch_data)  # Shape: (batch_size, 39)\n",
        "            probs = torch.softmax(logits, dim=1)  # Convert to probabilities\n",
        "\n",
        "            # Move back to CPU and store individual predictions\n",
        "            probs_cpu = probs.cpu()\n",
        "            for i in range(probs_cpu.shape[0]):\n",
        "                predictions.append(probs_cpu[i])  # Shape: (39,)\n",
        "\n",
        "    print(f\"Generated {len(predictions)} predictions\")\n",
        "    return predictions\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading phoneme holdout dataset...\")\n",
        "phoneme_dataset = LibriBrainCompetitionHoldout(\n",
        "    data_path=base_path,\n",
        "    task=\"phoneme\"\n",
        ")\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Generate predictions with the trained model\n",
        "print(\"Generating predictions with trained model...\")\n",
        "model_predictions = generate_model_predictions(\n",
        "    model=model,\n",
        "    dataset=phoneme_dataset,\n",
        "    batch_size=32,  # Adjust based on your GPU memory\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Create submission file\n",
        "submission_filename = \"trained_model_phoneme_submission.csv\"\n",
        "phoneme_dataset.generate_submission_in_csv(model_predictions, submission_filename)\n",
        "\n",
        "print(f\"\\n‚úÖ Trained model submission created: {submission_filename}\")\n",
        "print(f\"üìä Contains {len(model_predictions)} predictions\")\n",
        "print(\"üéØ Ready for upload to EvalAI!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDUhvUVojZ8a"
      },
      "source": [
        "Some things to note when using your own model:\n",
        "- Ensure your model takes input shape `(batch_size, 306, 125)`\n",
        "- Ensure your model outputs shape `(batch_size, 39)`\n",
        "- Remember to use `torch.softmax()` to convert logits to normalized probabilities during inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4n39JJmfIBA"
      },
      "source": [
        "## Ready to submit?\n",
        "After generating the predictions file, the next step is to submit it for evaluation. Don't worry, you are allowed to submit multiple times. Please, take a look at the [Submit on EvalAI](https://neural-processing-lab.github.io/2025-libribrain-competition/participate/#4-submit-on-evalai) section on the website to learn more.\n",
        "\n",
        "### Expected Submission Format\n",
        "Your CSV file should have the following structure:\n",
        "- **Header**: `segment_idx,phoneme_1,phoneme_2,...,phoneme_39`\n",
        "- **Each row**: One segment with 39 probability values (one for each phoneme class)\n",
        "- **Probabilities**: Should sum to 1.0 for each row (the model uses softmax to ensure this)\n",
        "- **Total rows**: Number of segments in the holdout dataset (14,053) + 1 header row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDGZ8LLzAMvb"
      },
      "source": [
        "## That's it! ü•≥\n",
        "Thanks for taking the time to look at and/or participate in our competition!\n",
        "\n",
        "If you have any open questions, please get in touch through [our Discord server](https://neural-processing-lab.github.io/2025-libribrain-competition/links/discord). Good luck with the competition!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}