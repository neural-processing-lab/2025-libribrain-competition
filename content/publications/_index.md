---
layout: "simple"
---

## Publications

The LibriBrain Competition and dataset are documented in the following publications:

### Competition Paper

**The 2025 PNPL Competition: Speech Detection and Phoneme Classification in the LibriBrain Dataset**

*Authors:* Gilad Landau, Miran Ã–zdogan, Gereon Elvers, Francesco Mantegna, Pratik Somaiya, Dulhan Jayalath, Luisa Kurth, Teyun Kwon, Brendan Shillingford, Greg Farquhar, Minqi Jiang, Karim Jerbi, Hamza Abdelhedi, Yorguin Mantilla Ramos, Caglar Gulcehre, Mark Woolrich, Natalie Voets, Oiwi Parker Jones

*Venue:* NeurIPS 2025 Competition Track

*Abstract:* The advance of speech decoding from non-invasive brain data holds the potential for profound societal impact. Among its most promising applications is the restoration of communication to paralysed individuals affected by speech deficits such as dysarthria, without the need for high-risk surgical interventions. The ultimate aim of the 2025 PNPL competition is to produce the conditions for an "ImageNet moment" or breakthrough in non-invasive neural decoding, by harnessing the collective power of the machine learning community.

To facilitate this vision we present the largest within-subject MEG dataset recorded to date (LibriBrain) together with a user-friendly Python library (pnpl) for easy data access and integration with deep learning frameworks. For the competition we define two foundational tasks (i.e. Speech Detection and Phoneme Classification from brain data), complete with standardised data splits and evaluation metrics, illustrative benchmark models, online tutorial code, a community discussion board, and public leaderboard for submissions. To promote accessibility and participation the competition features a Standard track that emphasises algorithmic innovation, as well as an Extended track that is expected to reward larger-scale computing, accelerating progress toward a non-invasive brain-computer interface for speech.

ðŸ“„ [Read on ArXiv](https://arxiv.org/abs/2506.10165v1)  
ðŸ“– [Download PDF](https://arxiv.org/pdf/2506.10165v1.pdf)

**Citation:**

{{< bibtex id="landau2025pnpl" title="Cite Competition Paper" >}}
@article{landau2025pnpl,
  title={The 2025 PNPL Competition: Speech Detection and Phoneme Classification in the LibriBrain Dataset},
  author={Landau, Gilad and Ã–zdogan, Miran and Elvers, Gereon and Mantegna, Francesco and Somaiya, Pratik and Jayalath, Dulhan and Kurth, Luisa and Kwon, Teyun and Shillingford, Brendan and Farquhar, Greg and others},
  journal={arXiv preprint arXiv:2506.10165},
  year={2025}
}
{{< /bibtex >}}

---

### Dataset Paper

**LibriBrain: Over 50 Hours of Within-Subject MEG to Improve Speech Decoding Methods at Scale**

*Authors:* Miran Ã–zdogan, Gilad Landau, Gereon Elvers, Dulhan Jayalath, Pratik Somaiya, Francesco Mantegna, Mark Woolrich, Oiwi Parker Jones

*Abstract:* LibriBrain represents the largest single-subject MEG dataset to date for speech decoding, with over 50 hours of recordings -- 5Ã— larger than the next comparable dataset and 50Ã— larger than most. This unprecedented 'depth' of within-subject data enables exploration of neural representations at a scale previously unavailable with non-invasive methods. LibriBrain comprises high-quality MEG recordings together with detailed annotations from a single participant listening to naturalistic spoken English, covering nearly the full Sherlock Holmes canon. Designed to support advances in neural decoding, LibriBrain comes with a Python library for streamlined integration with deep learning frameworks, standard data splits for reproducibility, and baseline results for three foundational decoding tasks: speech detection, phoneme classification, and word classification. Baseline experiments demonstrate that increasing training data yields substantial improvements in decoding performance, highlighting the value of scaling up deep, within-subject datasets. By releasing this dataset, we aim to empower the research community to advance speech decoding methodologies and accelerate the development of safe, effective clinical brain-computer interfaces.

ðŸ“„ [Read on ArXiv](https://arxiv.org/abs/2506.02098)  
ðŸ“– [Download PDF](https://arxiv.org/pdf/2506.02098.pdf)

**Citation:**

{{< bibtex id="ozdogan2025libribrain" title="Cite Dataset Paper" >}}
@article{ozdogan2025libribrain,
  title={LibriBrain: Over 50 Hours of Within-Subject MEG to Improve Speech Decoding Methods at Scale},
  author={Ã–zdogan, Miran and Landau, Gilad and Elvers, Gereon and Jayalath, Dulhan and Somaiya, Pratik and Mantegna, Francesco and Woolrich, Mark and Jones, Oiwi Parker},
  journal={arXiv preprint arXiv:2506.02098},
  year={2025}
}
{{< /bibtex >}}

---

 